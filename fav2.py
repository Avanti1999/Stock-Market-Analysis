# -*- coding: utf-8 -*-
"""FAV2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bOCQop0y1dvRKJuCC16YcbOFmQuZHuVV

**IMORTING ALL REQUIRED LIB**
"""

# Commented out IPython magic to ensure Python compatibility.
import yfinance as yf
from urllib.request import urlopen, Request
from bs4 import BeautifulSoup
import os
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
# NLTK VADER for sentiment analysis
import nltk
nltk.downloader.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

"""# **1. Create a model to predict next 3 day stock prices using historical data.**"""

#Define the ticker symbols and corresponding company names
ticker_symbols = ['SPGI', 'GRAB', 'AAPL', 'AMZN']
companies = ['S&P Global Inc.', 'Grab Holdings Limited', 'Apple Inc.', 'Amazon.com, Inc.']
dfs = [] #Initialize an empty list to store the DataFrames
for symbol, company in zip(ticker_symbols, companies):                    #Iterate through each ticker symbol and fetch historical data
    try:
        stock_data = yf.download(symbol, start='2024-02-01', end='2024-03-15')     # Fetch historical data
        stock_data['Company'] = company         #Add a 'Company' column with the company name
        dfs.append(stock_data)                  #Append the stock data df to the list
    except Exception as e:
        print(f"Error fetching data for {company}: {e}")
all_stock_data = pd.concat(dfs)              #Concatenate all DataFrames in the list into a single DataFrame

# Save the data to a CSV file
file_path = "all_stock_data.csv"
all_stock_data.to_csv(file_path)

print("Data saved to", file_path)

all_stock_data.head()

all_stock_data.tail()

all_stock_data.dropna(inplace=True)       #Cleand the data and handled missing values

stock_data.reset_index(inplace=True)

# Split data into features and target
X = all_stock_data.drop(columns=['Close','Company'], axis=1)  # Excluding 'Date' and 'Close' columns from features
y = all_stock_data['Close']

print(all_stock_data.columns)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)         #split data in train and test

print(X_train.shape )
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

model = LinearRegression()
model.fit(X_train, y_train)

NextThreeDayStock=stock_data.iloc[-1:].drop(columns=['Date', 'Close','Company'])

# Predict next 3-day stock prices
predictions_next_three_days = model.predict(NextThreeDayStock)
print("The model forecasts that the stock's price will be around", predictions_next_three_days ," per share over the upcoming three-day period.", )

"""# **3. Compare two or more companies based on their industry.**"""

#Select relevant financial metrics for comparison
selected_features=['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']

comparison_metrics =all_stock_data[selected_features]

print("Comparison of Companies Based on Industry:")
print(comparison_metrics)

"""# **2. Sentiment analysis on the news/twitter related to a particular stock.**"""

finwiz_url = 'https://finviz.com/quote.ashx?t='
news_tables = {}
tickers = ['AMZN']
for ticker in tickers:
    url = finwiz_url + ticker
    req = Request(url=url,headers={'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:20.0) Gecko/20100101 Firefox/20.0'})
    response = urlopen(req)
    # Read the contents of the file into 'html'
    html = BeautifulSoup(response)
    # Find 'news-table' in the Soup and load it into 'news_table'
    news_table = html.find(id='news-table')
    # Add the table to our dictionary
    news_tables[ticker] = news_table

# Read one single day of headlines for ‘AMZN’
amzn = news_tables['AMZN']
# Get all the table rows tagged in HTML with <tr> into ‘amzn_tr’
amzn_tr = amzn.findAll('tr')
for i, table_row in enumerate(amzn_tr):
    if table_row.a:
        a_text = table_row.a.text
    else:
        a_text = None
    td_text = table_row.td.text
    print(a_text)
    print(td_text)
    if i == 20:
        break

parsed_news = []

# Read one single day of headlines for ‘AMZN’
amzn = news_tables['AMZN']

# Get all the table rows tagged in HTML with <tr> into ‘amzn_tr’
amzn_tr = amzn.findAll('tr')

# Initialize variables to store date and time
current_date = None

# Iterate through the table rows
for table_row in amzn_tr:
    # Check if 'a' tag exists within the 'tr' tag
    if table_row.a:
        # Extract the headline text
        headline_text = table_row.a.text

        # Extract the time text
        time_text = table_row.td.text.strip()

        # Check if the headline has a 'date' label
        if 'Today' in time_text or 'Mar-' in time_text:
            # If it has a 'date' label, update the current date
            current_date = time_text

        # Append the parsed data into the list
        parsed_news.append([current_date, time_text, headline_text])

# Print the parsed news list
for news_item in parsed_news:
    print(news_item)

import pandas as pd
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Download NLTK Vader lexicon (if not already downloaded)
nltk.download('vader_lexicon')

# Initialize the Vader sentiment analyzer
sid = SentimentIntensityAnalyzer()

# Create an empty DataFrame to store the data
df = pd.DataFrame(columns=['Date', 'Time', 'Headline', 'Compound Score'])

# Iterate through the parsed news
for news_item in parsed_news:
    # Extract ticker, date, time, and headline from the parsed news item

    date = news_item[0]
    time = news_item[1]
    headline = news_item[2]

    # Perform sentiment analysis on the headline
    scores = sid.polarity_scores(headline)

    # Extract the compound score
    compound_score = scores['compound']

    # Append the data to the DataFrame
    df = df.append({'Date': date, 'Time': time, 'Headline': headline, 'Compound Score': compound_score}, ignore_index=True)

# Display the DataFrame
print(df)

# Categorize sentiment scores
def categorize_sentiment(score):
    if score > 0.05:
        return 'Positive'
    elif score < -0.05:
        return 'Negative'
    else:
        return 'Neutral'

# Apply the categorize_sentiment function to the 'Compound Score' column
df['Sentiment'] = df['Compound Score'].apply(categorize_sentiment)

# Display the modified DataFrame
print(df)

# Create new columns for positive, negative, and neutral sentiment scores
df['Positive Score'] = df['Compound Score'].apply(lambda score: score if score > 0.05 else None)
df['Negative Score'] = df['Compound Score'].apply(lambda score: score if score < -0.05 else None)
df['Neutral Score'] = df['Compound Score'].apply(lambda score: score if -0.05 <= score <= 0.05 else None)

# Convert None values to NaN for consistency
df['Positive Score'] = df['Positive Score'].replace({None: float('nan')})
df['Negative Score'] = df['Negative Score'].replace({None: float('nan')})
df['Neutral Score'] = df['Neutral Score'].replace({None: float('nan')})

# Display the modified DataFrame
print(df)

df.to_csv('sentiment_analysis_result.csv', index=False)

# Replace 'Today' in 'Date' and 'Time' columns with the current date
current_date = pd.to_datetime('today').strftime('%b-%d-%y')
df['Date'] = df['Date'].str.replace('Today', current_date)
df['Time'] = df['Time'].str.replace('Today', pd.to_datetime('today').strftime('%I:%M%p'))

# Convert time strings to 24-hour format
df['Time'] = pd.to_datetime(df['Time']).dt.strftime('%H:%M:%S')

# Concatenate 'Date' and 'Time' columns to create a datetime column
df['Datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])

# Drop the original 'Date' and 'Time' columns
df = df.drop(columns=['Date', 'Time'])

# Display the cleaned DataFrame
print(df)

df.isnull().sum()

#Fill missing values
df['Positive Score'] = df['Positive Score'].fillna(0)
df['Negative Score'] = df['Negative Score'].fillna(0)
df['Neutral Score'] = df['Neutral Score'].fillna(0)
df.head()

df.groupby('Sentiment').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

"""# **Make a Dashboard to produce comprehensive reports and compile the findings.**"""

!pip install dash

import pandas as pd
import dash
from dash import dcc
from dash import html
from dash.dependencies import Input, Output
import plotly.express as px

# Load the stock data
stock_data = pd.read_csv("all_stock_data.csv")

# Create a Dash app
app = dash.Dash(__name__)

# Define the layout of the app
app.layout = html.Div([
    html.H1("Stock Comparison Dashboard"),

    # Dropdown for selecting companies
    html.Label("Select Company:"),
    dcc.Dropdown(
        id='company-dropdown',
        options=[
            {'label': company, 'value': company} for company in stock_data['Company'].unique()
        ],
        value=stock_data['Company'].unique()[0],  # Default value
        multi=True  # Allow multiple selection
    ),

    # Line chart for stock prices
    dcc.Graph(id='stock-prices-graph'),

    # Table for displaying financial data
    html.H3("Financial Data"),
    html.Div(id='financial-data-table')
])

# Callback to update the line chart based on selected companies
@app.callback(
    Output('stock-prices-graph', 'figure'),
    [Input('company-dropdown', 'value')]
)
def update_stock_prices(selected_companies):
    filtered_data = stock_data[stock_data['Company'].isin(selected_companies)]
    fig = px.line(filtered_data, x='Date', y='Close', color='Company', title='Stock Prices Over Time')
    return fig

# Callback to update the financial data table based on selected companies
@app.callback(
    Output('financial-data-table', 'children'),
    [Input('company-dropdown', 'value')]
)
def update_financial_data(selected_companies):
    filtered_data = stock_data[stock_data['Company'].isin(selected_companies)]
    return generate_table(filtered_data)

# Function to generate an HTML table from a DataFrame
def generate_table(dataframe, max_rows=10):
    return html.Table([
        html.Thead(
            html.Tr([html.Th(col) for col in dataframe.columns])
        ),
        html.Tbody([
            html.Tr([
                html.Td(dataframe.iloc[i][col]) for col in dataframe.columns
            ]) for i in range(min(len(dataframe), max_rows))
        ])
    ])

# Run the app
if __name__ == '__main__':
    app.run_server(debug=True)